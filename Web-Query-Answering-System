# -*- coding: utf-8 -*-
"""FINAL-BERT-AND-GOOGLE-WITH-WIKI-FOR-Q-AND-A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1voAISAA7hp4inHDKt7DtuO0ACM55Jc1A
"""

!pip install google
!pip install wikipedia
!pip install transformers
import torch
import google
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer
from googlesearch import search
from collections import OrderedDict
import wikipedia

#Model
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
#Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question="who is the famous actor in india?"

wikipedia1=[]
W=[]
for i in search(question,stop=20, pause=2):
  k=i
  if("wikipedia" in k):
    wikipedia1.append(k)
  else:
    W.append(k)

F=wikipedia1[0].split("/")
mention=F[len(F)-1]

page=wikipedia.page(mention,auto_suggest=False)
text=page.content

inputs = tokenizer.encode_plus(question, text, return_tensors='pt')

qmask = inputs['token_type_ids'].lt(1)

qt = torch.masked_select(inputs['input_ids'], qmask)
print(f"The question consists of {qt.size()[0]} tokens.")

chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the "-1" accounts for
# having to add a [SEP] token to the end of each chunk
print(f"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.")

chunked_input = OrderedDict()
for k,v in inputs.items():
    q = torch.masked_select(v, qmask)
    c = torch.masked_select(v, ~qmask)
    chunks = torch.split(c, chunk_size)
 
    for i, chunk in enumerate(chunks):
        if i not in chunked_input:
            chunked_input[i] = {}
 
        thing = torch.cat((q, chunk))
        if i != len(chunks)-1:
            if k == 'input_ids':
                thing = torch.cat((thing, torch.tensor([102])))
            else:
                thing = torch.cat((thing, torch.tensor([1])))
 
        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)

for i in range(len(chunked_input.keys())):
    print(f"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}")

def convert_ids_to_string(tokenizer, input_ids):
    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))
 
answer = ''
 
# now we iterate over our chunks, looking for the best answer from each chunk
for _, chunk in chunked_input.items():
    answer_start_scores, answer_end_scores = model(**chunk)
 
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1
 
    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])
    
    # if the ans == [CLS] then the model did not find a real answer in this chunk
    if ('[CLS]' not in ans) and ('[SEP]' not in ans) :
        answer += ans + " / "
        
print(answer)
print("You can Visit this for more information: ",wikipedia1[0])

